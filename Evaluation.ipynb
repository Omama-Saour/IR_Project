{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63d5c30",
   "metadata": {},
   "source": [
    "# Query Processing ðŸŒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb470780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell checking\n",
    "# pip install pyspellchecker \n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List  # Import the List type from the typing module\n",
    "import re #to remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class QueryProcessing:\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def correct_sentence_spelling(tokens: List[str]) -> List[str]:\n",
    "        spell = SpellChecker()\n",
    "        misspelled = spell.unknown(tokens)\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in misspelled:\n",
    "                corrected = spell.correction(token)\n",
    "                if corrected is not None:\n",
    "                    tokens[i] = corrected\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def query_processing(query_text):\n",
    "\n",
    "    # Normalization\n",
    "        query_text = re.sub(r'\\W', ' ', str(query_text)) # Replace non-word characters with a space\n",
    "        query_text = re.sub(r'\\s+', ' ', query_text)  # Remove extra spaces\n",
    "\n",
    "        # Convert the entire document to lowercase\n",
    "        query_text = query_text.lower() \n",
    "\n",
    "        # Tokenize into words\n",
    "        query_text_words = word_tokenize(query_text)\n",
    "        query_text_words = QueryProcessing.correct_sentence_spelling(query_text_words)\n",
    " \n",
    "        # Stemming      \n",
    "        query_text_stemmed_words = [QueryProcessing.stemmer.stem(word) for word in query_text_words] \n",
    "        \n",
    "        # Lemmatization\n",
    "        query_text_Lemmatized_words = [QueryProcessing.lemmatizer.lemmatize(word) for word in query_text_stemmed_words]\n",
    "       \n",
    "        # Normalization\n",
    "        # Remove stopwords from the text \n",
    "        query_text_words = [word for word in query_text_Lemmatized_words if word not in stopwords.words('English')]\n",
    "\n",
    "        removed_punctuation_query = [word for word in query_text_words if word.isalnum()]\n",
    "\n",
    "        return removed_punctuation_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02bf3e",
   "metadata": {},
   "source": [
    "# Matching & Ranking ðŸŒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea18dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from QueryProcessing import QueryProcessing\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "class MatchingRanking:\n",
    "\n",
    "    # Convert the documents into a list of strings\n",
    "    df = pd.read_csv('recreation-collection.csv',encoding='latin-1')\n",
    "    document_strings = []\n",
    "    for document in df['doc']:\n",
    "        document_strings.append(document)\n",
    "    # print(f\"The size of the document_strings is: {len(document_strings)}\")\n",
    "\n",
    "\n",
    "    document_IDs = []\n",
    "    for document_id in df['num']:\n",
    "        document_IDs.append(document_id)\n",
    "    # print(f\"The size of the document_IDs is: {len(document_IDs)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def matching_and_ranking(query_text):\n",
    "        # Load tfidf_matrix from the binary file\n",
    "        with open('tfidf_matrix.bin', 'rb') as file:\n",
    "            tfidf_matrix = pickle.load(file)\n",
    "            # print(\"tfidf_matrix: \")\n",
    "            # print(tfidf_matrix.shape)\n",
    "\n",
    "        # Load the model\n",
    "        with open('model.pkl', 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "            \n",
    "        # Preprocess the query\n",
    "        queryProcessing_instance = QueryProcessing()\n",
    "        query = queryProcessing_instance.query_processing(query_text) \n",
    "\n",
    "        # Transform the query into a vector\n",
    "        query_string = ' '.join(query)  # Convert the list of tokens back into a single string\n",
    "\n",
    "        # Convert the query document to a TF-IDF vector\n",
    "        query_vector = vectorizer.transform([query_string])\n",
    "\n",
    "        # Calculate cosine similarity between the query vector and document vectors\n",
    "        cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "        # Rank the results\n",
    "        document_indices = np.argsort(cosine_similarities)[::-1]\n",
    "        k = 10\n",
    "        ranked_documents = document_indices[:k]  \n",
    "\n",
    "        ranked_document_ids = []\n",
    "\n",
    "        # Print the ranked documents\n",
    "        for idx in ranked_documents:\n",
    "            ranked_document_ids.append(MatchingRanking.document_IDs[idx])\n",
    "\n",
    "        return ranked_document_ids\n",
    "\n",
    "# Call the matching_and_ranking() method to execute the code\n",
    "# MatchingRanking.matching_and_ranking(\"real world we dont use most of it\")\n",
    "    \n",
    "# First Data\n",
    "# qrels = Evaluation.load_qrels(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\antique DataSet\\\\antique-test.qrel\")\n",
    "# queries = Evaluation.load_queries(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\antique DataSet\\\\antique-test-queries.txt\")\n",
    "\n",
    "# ranked_doc_ids = {}\n",
    "# for query_id, query_text in queries.items():\n",
    "#     ranked_doc_ids[query_id] = MatchingRanking.matching_and_ranking(query_text)\n",
    "    \n",
    "# Second Data\n",
    "qrels_recreation = Evaluation.load_qrels_recreation(\"C:\\\\Users\\\\Asus\\\\Desktop\\\\project 2-6\\\\project 2-6\\\\qas.search.jsonl\")\n",
    "queries_recreation = Evaluation.load_queries_recreation(\"C:\\\\Users\\\\Asus\\\\Desktop\\\\project 2-6\\\\project 2-6\\\\questions.search.txt\")\n",
    "\n",
    "ranked_doc_ids_recreation = {}\n",
    "for query_id, query_text in queries_recreation.items():\n",
    "    ranked_doc_ids_recreation[query_id] = MatchingRanking.matching_and_ranking(query_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad912b",
   "metadata": {},
   "source": [
    "# Evaluation ðŸŒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bdb075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatchingRanking import MatchingRanking\n",
    "import json\n",
    "\n",
    "class Evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_qrels(file_path):\n",
    "        qrels = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                query_id, _, doc_id, relevance = line.strip().split()\n",
    "                if query_id not in qrels:\n",
    "                    qrels[query_id] = {}\n",
    "                qrels[query_id][doc_id] = int(relevance)\n",
    "        return qrels\n",
    "\n",
    "    @staticmethod\n",
    "    def load_queries(file_path):\n",
    "        queries = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                query_id, query_text = line.strip().split('\\t')\n",
    "                queries[query_id] = query_text\n",
    "        return queries\n",
    "\n",
    "\n",
    "    @staticmethod    \n",
    "    def load_qrels_recreation(file_path):\n",
    "        qrels = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f: \n",
    "                try:\n",
    "                    # Attempt to parse each line as JSON\n",
    "                    data = json.loads(line.strip())  # Parse each line as separate JSON\n",
    "                    query_id =  str(data['qid'])\n",
    "                    for doc_id in data['answer_pids']:\n",
    "                        relevance = 1  # Assuming relevance is always 1 in this format\n",
    "                        if query_id not in qrels:\n",
    "                            qrels[query_id] = {}\n",
    "                        qrels[query_id][doc_id] = relevance\n",
    "                    relevance = 1  # Assuming relevance is always 1 in this format\n",
    "                except json.JSONDecodeError:\n",
    "                    # Handle potential non-JSON lines\n",
    "                    continue  # Skip to the next line\n",
    "\n",
    "                if query_id not in qrels:\n",
    "                    qrels[query_id] = {}\n",
    "                qrels[query_id][doc_id] = relevance\n",
    "        return qrels\n",
    "\n",
    "    @staticmethod\n",
    "    def load_queries_recreation(file_path):\n",
    "        queries = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                query_id, query_text = line.strip().split('\\t')\n",
    "                queries[query_id] = query_text\n",
    "        return queries\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_precision_at_k(qrels, queries, ranked_doc_ids, k=10):\n",
    "        avg_precision_at_k = 0\n",
    "        num_queries = 0\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            if query_id not in qrels:\n",
    "                print(f\"No relevance judgments found for query {query_id}.\")\n",
    "                continue\n",
    "\n",
    "            relevant_docs_at_k = 0\n",
    "            for i, doc_id in enumerate(ranked_doc_ids[query_id]):\n",
    "                if i >= k:\n",
    "                    break\n",
    "                if doc_id in qrels[query_id] and qrels[query_id][doc_id] >= 1:\n",
    "                    relevant_docs_at_k += 1\n",
    "\n",
    "            precision_at_k = relevant_docs_at_k / k\n",
    "            avg_precision_at_k += precision_at_k\n",
    "            num_queries += 1\n",
    "\n",
    "        if num_queries > 0:\n",
    "            avg_precision_at_k /= num_queries\n",
    "        return avg_precision_at_k\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall_at_k(qrels, queries, ranked_doc_ids, k=10, relevance_threshold=8):\n",
    "        avg_recall_at_k = 0\n",
    "        num_queries = 0\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            if query_id not in qrels:\n",
    "                print(f\"No relevance judgments found for query {query_id}.\")\n",
    "                continue\n",
    "\n",
    "            relevant_docs_at_k = 0\n",
    "            for i, doc_id in enumerate(ranked_doc_ids[query_id]):\n",
    "                if i >= k:\n",
    "                    break\n",
    "                if doc_id in qrels[query_id] and qrels[query_id][doc_id] >= 1:\n",
    "                    relevant_docs_at_k += 1\n",
    "\n",
    "            recall_at_k = relevant_docs_at_k / relevance_threshold\n",
    "            avg_recall_at_k += recall_at_k\n",
    "            num_queries += 1\n",
    "\n",
    "        if num_queries > 0:\n",
    "            avg_recall_at_k /= num_queries\n",
    "        return avg_recall_at_k\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_average_precision(qrels, queries, ranked_doc_ids):\n",
    "        average_precisions = []\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            if query_id not in qrels:\n",
    "                print(f\"No relevance judgments found for query {query_id}.\")\n",
    "                continue\n",
    "\n",
    "            relevant_docs = 0\n",
    "            precision_values = []\n",
    "\n",
    "            for i, doc_id in enumerate(ranked_doc_ids[query_id]):\n",
    "                if doc_id in qrels[query_id] and qrels[query_id][doc_id] >= 1:\n",
    "                    relevant_docs += 1\n",
    "                    precision = relevant_docs / (i + 1)\n",
    "                    precision_values.append(precision)\n",
    "\n",
    "            if relevant_docs > 0:\n",
    "                average_precision = sum(precision_values) / relevant_docs\n",
    "                average_precisions.append(average_precision)\n",
    "\n",
    "        return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_reciprocal_rank(qrels, queries, ranked_doc_ids):\n",
    "        reciprocal_ranks = []\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            if query_id not in qrels:\n",
    "                print(f\"No relevance judgments found for query {query_id}.\")\n",
    "                continue\n",
    "\n",
    "            found_relevant = False\n",
    "            rank = 1\n",
    "\n",
    "            for doc_id in ranked_doc_ids[query_id]:\n",
    "                if doc_id in qrels[query_id] and qrels[query_id][doc_id] >= 1:\n",
    "                    reciprocal_rank = 1 / rank\n",
    "                    reciprocal_ranks.append(reciprocal_rank)\n",
    "                    found_relevant = True\n",
    "                    break\n",
    "                rank += 1\n",
    "\n",
    "            if not found_relevant:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "\n",
    "        return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cdae3",
   "metadata": {},
   "source": [
    "## First Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7f1ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@10 antique: 0.5845\n",
      "Average Recall@10 antique: 0.5675\n",
      "Mean Average Precision (MAP) antique: 0.6855\n",
      "Mean Reciprocal Rank (MRR) antique: 0.6465\n"
     ]
    }
   ],
   "source": [
    "avg_p_at_10 = Evaluation.calculate_precision_at_k(qrels, queries, ranked_doc_ids, k=10)\n",
    "print(f\"Average Precision@10 antique: {avg_p_at_10:.4f}\")\n",
    "\n",
    "avg_recall_at_10 = Evaluation.calculate_recall_at_k(qrels, queries, ranked_doc_ids, k=10)\n",
    "print(f\"Average Recall@10 antique: {avg_recall_at_10:.4f}\")\n",
    "\n",
    "map_score = Evaluation.calculate_mean_average_precision(qrels, queries, ranked_doc_ids)\n",
    "print(f\"Mean Average Precision (MAP) antique: {map_score:.4f}\")\n",
    "\n",
    "mrr_score = Evaluation.calculate_mean_reciprocal_rank(qrels, queries, ranked_doc_ids)\n",
    "print(f\"Mean Reciprocal Rank (MRR) antique: {mrr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59438e4c",
   "metadata": {},
   "source": [
    "## Second Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f72d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@10 recreation: 0.4833\n",
      "Average Recall@10 recreation: 0.4609\n",
      "Mean Average Precision (MAP) recreation: 0.4709\n",
      "Mean Reciprocal Rank (MRR) recreation: 0.1753\n"
     ]
    }
   ],
   "source": [
    "avg_p_at_10_recreation = Evaluation.calculate_precision_at_k(qrels_recreation, queries_recreation, ranked_doc_ids_recreation, k=10)\n",
    "print(f\"Average Precision@10 recreation: {avg_p_at_10_recreation:.4f}\")\n",
    "\n",
    "avg_recall_at_10_recreation = Evaluation.calculate_recall_at_k(qrels_recreation, queries_recreation, ranked_doc_ids_recreation, k=10)\n",
    "print(f\"Average Recall@10 recreation: {avg_recall_at_10_recreation:.4f}\")\n",
    "\n",
    "map_score_recreation = Evaluation.calculate_mean_average_precision(qrels_recreation, queries_recreation, ranked_doc_ids_recreation)\n",
    "print(f\"Mean Average Precision (MAP) recreation: {map_score_recreation:.4f}\")\n",
    "\n",
    "mrr_score_recreation = Evaluation.calculate_mean_reciprocal_rank(qrels_recreation, queries_recreation, ranked_doc_ids_recreation)\n",
    "print(f\"Mean Reciprocal Rank (MRR) recreation: {mrr_score_recreation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c9693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
